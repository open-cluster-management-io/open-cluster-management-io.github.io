<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>User Scenarios on Open Cluster Management</title><link>https://open-cluster-management.io/docs/scenarios/</link><description>Recent content in User Scenarios on Open Cluster Management</description><generator>Hugo</generator><language>en</language><atom:link href="https://open-cluster-management.io/docs/scenarios/index.xml" rel="self" type="application/rss+xml"/><item><title>Deploy Kubernetes resources to the managed clusters</title><link>https://open-cluster-management.io/docs/scenarios/deploy-kubernetes-resources/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-cluster-management.io/docs/scenarios/deploy-kubernetes-resources/</guid><description>&lt;p>After bootstrapping an OCM environment of at least one managed clusters, now
it&amp;rsquo;s time to begin your first journey of deploying Kubernetes resources into
your managed clusters with OCM&amp;rsquo;s &lt;code>ManifestWork&lt;/code> API.&lt;/p>
&lt;h2 id="prerequisites">Prerequisites&lt;/h2>
&lt;p>Before we get start with the following tutorial, let&amp;rsquo;s clarify a few terms
we&amp;rsquo;re going to use in the context.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Cluster namespace&lt;/strong>: After a managed cluster is successfully registered
into the hub. The hub registration controller will be automatically
provisioning a &lt;code>cluster namespace&lt;/code> dedicated for the cluster of which the
name will be same as the managed cluster. The &lt;code>cluster namespace&lt;/code> is used
for storing any custom resources/configurations that effectively belongs
to the managed cluster.&lt;/p></description></item><item><title>Distribute workload with placement selected managed clusters</title><link>https://open-cluster-management.io/docs/scenarios/distribute-workload-with-placement/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-cluster-management.io/docs/scenarios/distribute-workload-with-placement/</guid><description>&lt;p>The &lt;code>Placement&lt;/code> API is used to dynamically select a set of &lt;code>ManagedCluster&lt;/code> in
one or multiple &lt;code>ManagedClusterSets&lt;/code> so that the workloads can be deployed to
these clusters.&lt;/p>
&lt;p>If you define a valid &lt;code>Placement&lt;/code>, the placement controller generates a
corresponding &lt;code>PlacementDecision&lt;/code> with the selected clusters listed in the
status. As an end-user, you can parse the selected clusters and then operate on
the target clusters. You can also integrate a high-level workload orchestrator
with the &lt;code>PlacementDecision&lt;/code> to leverage its scheduling capabilities.&lt;/p></description></item><item><title>Extend the multicluster scheduling capabilities with placement</title><link>https://open-cluster-management.io/docs/scenarios/extend-multicluster-scheduling-capabilities/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-cluster-management.io/docs/scenarios/extend-multicluster-scheduling-capabilities/</guid><description>&lt;p>The &lt;code>Placement&lt;/code> API is used to dynamically select a set of &lt;code>ManagedCluster&lt;/code> in one or multiple &lt;code>ManagedClusterSets&lt;/code> so that the workloads can be deployed to these clusters. You can use placement to filter clusters by label or claim selector, also placement provides some default prioritizers which can be used to sort and select the most suitable clusters.&lt;/p>
&lt;p>One of the default prioritizers are ResourceAllocatableCPU and ResourceAllocatableMemory. They provide the capability to sort clusters based on the allocatable CPU and memory. However, when considering the resource based scheduling, there&amp;rsquo;s a gap that the cluster’s &amp;ldquo;AllocatableCPU&amp;rdquo; and &amp;ldquo;AllocatableMemory&amp;rdquo; are static values that won’t change even if &amp;ldquo;the cluster is running out of resources&amp;rdquo;. And in some cases, the prioritizer needs more extra data to calculate the score of the managed cluster. For example, there is a requirement to schedule based on resource monitoring data from the cluster. For this reason, we need a more extensible way to support scheduling based on customized scores.&lt;/p></description></item><item><title>Extending managed clusters with custom attributes</title><link>https://open-cluster-management.io/docs/scenarios/extending-managed-clusters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-cluster-management.io/docs/scenarios/extending-managed-clusters/</guid><description>&lt;p>Under some cases we need a convenient way to extend OCM&amp;rsquo;s &lt;a href="https://open-cluster-management.io/docs/concepts/cluster-inventory/managedcluster/">Managed Cluster&lt;/a>
data model so that our own custom multi-cluster system can easily work over the
OCM&amp;rsquo;s native cluster api otherwise we will have to maintain an additional
Kubernetes&amp;rsquo; &lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/">CustomResourceDefinition&lt;/a>
in the project. OCM definitely supports developers to decorate the cluster api
with minimal effort, and in the following content we will walk through that
briefly.&lt;/p>
&lt;p>The original cluster model in OCM &amp;ldquo;Managed Cluster&amp;rdquo; is designed to be a
&lt;strong>neat&lt;/strong> and &lt;strong>light-weight&lt;/strong> placeholder resource of which the spec doesn&amp;rsquo;t
require any additional information other than &amp;ldquo;whether the cluster is
accepted or not&amp;rdquo; i.e. &lt;code>.spec.hubAcceptsClient&lt;/code>, and all the other fields
in the spec are totally optional, e.g. &lt;code>.spec.managedClusterClientConfigs&lt;/code>
is only required until we install some addons that replying on that
information.&lt;/p></description></item><item><title>Integration with Argo CD</title><link>https://open-cluster-management.io/docs/scenarios/integration-with-argocd/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-cluster-management.io/docs/scenarios/integration-with-argocd/</guid><description>&lt;p>&lt;a href="https://argo-cd.readthedocs.io/en/stable/">Argo CD&lt;/a> is a declarative, GitOps continuous delivery tool, which allows developers to define and control deployment of Kubernetes application resources from within their existing Git workflow. By integrating Open Cluster Management (OCM) with Argo CD, it enables both automation and greater flexibility managing Argo CD Applications across a large number of OCM managed clusters.&lt;/p>
&lt;p>In this article, we want to show you how to integrate Argo CD with OCM and deploy application to OCM managed clusters by leveraging the &lt;code>Placement&lt;/code> API, which supports multi-cluster scheduling.&lt;/p></description></item><item><title>Manage a cluster with multiple hubs</title><link>https://open-cluster-management.io/docs/scenarios/manage-cluster-with-multiple-hubs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-cluster-management.io/docs/scenarios/manage-cluster-with-multiple-hubs/</guid><description>&lt;p>Normally an Open Cluster Management (OCM) hub manages multiple managed clusters and a cluster only registers to one OCM hub. While there might be some user scenarios, where a single cluster may want to join more than one OCM hub as a managed cluster, including:&lt;/p>
&lt;ul>
&lt;li>In an organization, each department may setup an OCM hub to manage clusters owned by this department, and all clusters are managed by a central OCM hub owned by IT department to enforce organization wide security policies.&lt;/li>
&lt;li>A service provider creates clusters for customers. The underlying system of the service provider uses OCM hubs to manage all the clusters. Once customer gets a cluster from the service provider, they may also want to manage this cluster with customer&amp;rsquo;s OCM hub.&lt;/li>
&lt;/ul>
&lt;p>This document shows how to achieve it with OCM.&lt;/p></description></item><item><title>Migrate workload with placement</title><link>https://open-cluster-management.io/docs/scenarios/migrate-workload-with-placement/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-cluster-management.io/docs/scenarios/migrate-workload-with-placement/</guid><description>&lt;p>The &lt;code>Placement&lt;/code> API is used to dynamically select a set of &lt;code>ManagedCluster&lt;/code> in
one or multiple &lt;code>ManagedClusterSets&lt;/code> so that the workloads can be deployed to
these clusters.&lt;/p>
&lt;p>If you define a valid &lt;code>Placement&lt;/code>, the placement controller generates a
corresponding &lt;code>PlacementDecision&lt;/code> with the selected clusters listed in the
status. As an end-user, you can parse the selected clusters and then operate on
the target clusters. You can also integrate a high-level workload orchestrator
with the &lt;code>PlacementDecision&lt;/code> to leverage its scheduling capabilities.&lt;/p></description></item><item><title>Pushing Kubernetes API requests to the managed clusters</title><link>https://open-cluster-management.io/docs/scenarios/pushing-kube-api-requests/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-cluster-management.io/docs/scenarios/pushing-kube-api-requests/</guid><description>&lt;p>By following the instructions in this document, an OCM hub admin will be able
to &amp;ldquo;push&amp;rdquo; Kubernetes API requests to the managed clusters. The benefit of using
this method for &amp;ldquo;pushing&amp;rdquo; requests in OCM is that we don&amp;rsquo;t need to explicitly
configure any API endpoint for the managed clusters or provide any client
credentials as preparation. We just need to enable/install the following OCM
addons:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/open-cluster-management-io/cluster-proxy">Cluster-Proxy&lt;/a>:
Setting up the &lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/setup-konnectivity/">konnectivity&lt;/a>
tunnels between the hub cluster and the managed clusters so the hub cluster
can connect/access the managed cluster from anywhere.&lt;/li>
&lt;li>&lt;a href="https://github.com/open-cluster-management-io/managed-serviceaccount">Managed-ServiceAccount&lt;/a>:
Automating the lifecycle of the local service account in the managed clusters
and projecting the tokens back to the hub cluster so that the Kubernetes API
clients from the hub can make authenticated requests.&lt;/li>
&lt;li>&lt;a href="https://github.com/oam-dev/cluster-gateway">Cluster-Gateway&lt;/a>: An aggregated
apiserver providing a &amp;ldquo;proxy&amp;rdquo; subresource which helps the hub admin to
gracefully access the managed clusters by standard Kubernetes API calls
(including long-running calls).&lt;/li>
&lt;/ul>
&lt;h2 id="prerequisite">Prerequisite&lt;/h2>
&lt;p>You must meet the following prerequisites to install the managed service
account:&lt;/p></description></item><item><title>Register a cluster to hub through proxy server</title><link>https://open-cluster-management.io/docs/scenarios/register-cluster-through-proxy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-cluster-management.io/docs/scenarios/register-cluster-through-proxy/</guid><description>&lt;p>When registering a cluster to an Open Cluster Management (OCM) hub, there is a network requirement for the managed cluster. It must be able to reach the hub cluster. Sometimes the managed cluster cannot directly connect to the hub cluster. For example, the hub cluster is in a public cloud, and the managed cluster is in a private cloud environment behind firewalls. The communications out of the private cloud can only go through a HTTP or HTTPS proxy server.&lt;/p></description></item></channel></rss>